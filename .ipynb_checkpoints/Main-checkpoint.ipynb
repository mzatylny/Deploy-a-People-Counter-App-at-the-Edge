{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'prob_threshold' is assigned to before global declaration (<ipython-input-1-0088bd91b412>, line 337)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0088bd91b412>\"\u001b[0;36m, line \u001b[0;32m337\u001b[0m\n\u001b[0;31m    initial_h = cap.get(4)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'prob_threshold' is assigned to before global declaration\n"
     ]
    }
   ],
   "source": [
    "\"\"\"People Counter.\"\"\"\n",
    "\"\"\"\n",
    " Copyright (c) 2018 Intel Corporation.\n",
    " Permission is hereby granted, free of charge, to any person obtaining\n",
    " a copy of this software and associated documentation files (the\n",
    " \"Software\"), to deal in the Software without restriction, including\n",
    " without limitation the rights to use, copy, modify, merge, publish,\n",
    " distribute, sublicense, and/or sell copies of the Software, and to\n",
    " permit person to whom the Software is furnished to do so, subject to\n",
    " the following conditions:\n",
    " The above copyright notice and this permission notice shall be\n",
    " included in all copies or substantial portions of the Software.\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    " EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    " MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    " NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    " LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    " OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    " WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import socket\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import logging as log\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from inference import Network\n",
    "\n",
    "# MQTT server environment variables\n",
    "HOSTNAME = socket.gethostname()\n",
    "IPADDRESS = socket.gethostbyname(HOSTNAME)\n",
    "MQTT_HOST = IPADDRESS\n",
    "MQTT_PORT = 3001\n",
    "MQTT_KEEPALIVE_INTERVAL = 60\n",
    "\n",
    "\n",
    "def build_argparser():\n",
    "    \"\"\"\n",
    "    Parse command line arguments.\n",
    "\n",
    "    :return: command line arguments\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--model\", required=True, type=str,\n",
    "                        help=\"Path to an xml file with a trained model.\")\n",
    "    parser.add_argument(\"-i\", \"--input\", required=True, type=str,\n",
    "                        help=\"Path to image or video file\")\n",
    "    parser.add_argument(\"-l\", \"--cpu_extension\", required=False, type=str,\n",
    "                        default=None,\n",
    "                        help=\"MKLDNN (CPU)-targeted custom layers.\"\n",
    "                             \"Absolute path to a shared library with the\"\n",
    "                             \"kernels impl.\")\n",
    "    parser.add_argument(\"-d\", \"--device\", type=str, default=\"CPU\",\n",
    "                        help=\"Specify the target device to infer on: \"\n",
    "                             \"CPU, GPU, FPGA or MYRIAD is acceptable. Sample \"\n",
    "                             \"will look for a suitable plugin for device \"\n",
    "                             \"specified (CPU by default)\")\n",
    "    parser.add_argument(\"-pt\", \"--prob_threshold\", type=float, default=0.5,\n",
    "                        help=\"Probability threshold for detections filtering\"\n",
    "                        \"(0.5 by default)\")    \n",
    "    parser.add_argument(\"-c\", \"--color\", default='BLUE', \n",
    "                        help=\"The color of the bounding boxes to draw; RED, GREEN or BLUE\")\n",
    "    parser.add_argument(\"-ct\",\"--confidence_threshold\",type=float, default=0.5,\n",
    "                        help=\"The confidence threshold to use with the bounding boxes\")\n",
    "    parser.add_argument(\"-pc\", \"--perf_counts\", type=str, default=False,\n",
    "                        help=\"Print performance counters\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "########################################################################\n",
    "###########  conection #################################################\n",
    "########################################################################\n",
    "def connect_mqtt():\n",
    "    ### TODO: Connect to the MQTT client ###\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_HOST, MQTT_PORT, MQTT_KEEPALIVE_INTERVAL)\n",
    "\n",
    "    return client\n",
    "########################################################################\n",
    "###########  ssd_out #################################################\n",
    "########################################################################\n",
    "\n",
    "def ssd_out(frame, result):\n",
    "    \"\"\"\n",
    "    Parse SSD output.\n",
    "\n",
    "    :param frame: frame from camera/video\n",
    "    :param result: list contains the data to parse ssd\n",
    "    :return: person count and frame\n",
    "    \"\"\"\n",
    "    current_count = 0\n",
    "    for obj in result[0][0]:\n",
    "        # Draw bounding box for object when it's probability is more than\n",
    "        #  the specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 0, 255), 1)\n",
    "            current_count = current_count + 1\n",
    "    return frame, current_count\n",
    "\n",
    "\"\"\"\n",
    "#  publish the statistics to the connected client.\n",
    "topic = \"some_string\"\n",
    "client.publish(topic, json.dumps({\"stat_name\": statistic}))\n",
    "\n",
    "#  \n",
    "client.publish(\"class\", json.dumps({\"class_names\": class_names}))\n",
    "client.publish(\"speedometer\", json.dumps({\"speed\": speed}))\n",
    "\n",
    "#  disconect\n",
    "client.disconnect()\n",
    "\"\"\"\n",
    "\n",
    "#######################################\n",
    "# FFmpeg\n",
    "# FFmpeg en realidad no tiene ninguna importación específica real,\n",
    "# aunque sí queremos la sysbiblioteca estándar\n",
    "\n",
    "#import sys\n",
    "\n",
    "# Esto se usa ya que ffserverse puede configurar para leer sys.stdout. Una vez que se ha procesado \n",
    "# el marco de salida (dibujando cuadros delimitadores, máscaras semánticas, etc.), puede escribir\n",
    "# el marco en el stdoutbúfer y en flushél.\n",
    "\n",
    "#sys.stdout.buffer.write(frame)  \n",
    "#sys.stdout.flush()\n",
    "#######################################\n",
    "# runing the app\n",
    "# python app.py | ffmpeg -v warning -f rawvideo -pixel_format bgr24 -video_size 1280x720 \n",
    "#                        -framerate 24 -i - http://0.0.0.0:3004/fac.ffm\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\"\"\" \n",
    "def convert_color(color_string):\n",
    "    '''\n",
    "    Get the BGR value of the desired bounding box color.\n",
    "    Defaults to Blue if an invalid color is given.\n",
    "    '''\n",
    "    colors = {\"BLUE\": (255,0,0), \"GREEN\": (0,255,0), \"RED\": (0,0,255)}\n",
    "    out_color = colors.get(color_string)\n",
    "    if out_color:\n",
    "        return out_color\n",
    "    else:\n",
    "        return colors['BLUE']\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def draw_boxes(frame, result, args, width, height):\n",
    "\n",
    "    '''\n",
    "    Draw bounding boxes onto the frame.\n",
    "    '''\n",
    "    for box in result[0][0]: # Output shape is 1x1x100x7\n",
    "        conf = box[2]\n",
    "        if conf >= args.ct:\n",
    "            xmin = int(box[3] * width)\n",
    "            ymin = int(box[4] * height)\n",
    "            xmax = int(box[5] * width)\n",
    "            ymax = int(box[6] * height)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), args.c, 1)\n",
    "    return frame\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def performance_counts(perf_count):\n",
    "    \"\"\"\n",
    "    print information about layers of the model.\n",
    "\n",
    "    :param perf_count: Dictionary consists of status of the layers.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(\"{:<70} {:<15} {:<15} {:<15} {:<10}\".format('name', 'layer_type',\n",
    "                                                      'exec_type', 'status',\n",
    "                                                      'real_time, us'))\n",
    "    for layer, stats in perf_count.items():\n",
    "        print(\"{:<70} {:<15} {:<15} {:<15} {:<10}\".format(layer,\n",
    "                                                          stats['layer_type'],\n",
    "                                                          stats['exec_type'],\n",
    "                                                          stats['status'],\n",
    "                                                          stats['real_time']))\n",
    "\n",
    "\"\"\"\n",
    "def capture_stream(args):\n",
    "    ### TODO: Handle image, video or webcam\n",
    "    image_flag = None \n",
    "    #check if imput is a cam \n",
    "    if args.i ==\"CAM\":\n",
    "        args.i = 0 \n",
    "    elif args.i.endswith(\".jpg\") or args.i.endwith(\".bmp\"):\n",
    "        image_flag = True \n",
    "\n",
    "    ### TODO: Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "    \n",
    "    if not image_flag:\n",
    "        out = cv2.VideoWriter(\"out.mp4\",cv2.VideoWriter_fourcc(\"M\",\"J\",\"P\",\"G\"),30,(100,100))\n",
    "    else:\n",
    "        out = None     \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "    \n",
    "\n",
    "    ### TODO: Re-size the frame to 100x100\n",
    "        image = cv2.resize(frame, (100, 100))\n",
    "\n",
    "    ### TODO: Add Canny Edge Detection to the frame, \n",
    "    ###       with min & max values of 100 and 200\n",
    "    ###       Make sure to use np.dstack after to make a 3-channel image\n",
    "        edges = cv2.Canny(image,100,200)\n",
    "\n",
    "    ### TODO: Write out the frame, depending on image or video\n",
    "        if image_flag:\n",
    "            cv2.imwrite(\"out_img.jpg\",frame)\n",
    "        else:\n",
    "            out.write(frame)\n",
    "            \n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "    ### TODO: Close the stream and any windows at the end of the application\n",
    "    if not image_flag:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\"\n",
    "############################\n",
    "\"\"\" video capture stream\n",
    "\n",
    "capture = cv2.VideoCapture(input_stream)\n",
    "capture.open(args.input)\n",
    "\n",
    "while capture.isOpened():\n",
    "    flag, frame = cap.read()\n",
    "    if not flag:\n",
    "        break\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# to check for a key press \n",
    "key_pressed = cv2.waitKey(60)\n",
    "# and then\n",
    "if key_pressed == 27:\n",
    "    break\n",
    "# resize the frame to 100x100\n",
    "image = cv2.resize(frame, (100, 100))\n",
    "# Add Canny Edge Detection to the frame with min & max values of 100 and 200, respectively\n",
    "edges = cv2.Canny(image,100,200) # Canny Edge detection is useful for detecting edges in an image,\n",
    "and has been a useful computer vision technique for extracting features\n",
    "\n",
    "# Display the resulting frame if it's video, or save it if it is an image\n",
    "cv2.imshow('display', edges) # For video:\n",
    "# for a single image\n",
    "cv2.imwrite('output.jpg', edges)\n",
    "\n",
    "# make sure to destroy the windows \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n",
    "############################\n",
    "def infer_on_stream(args, client):\n",
    "    \"\"\"\n",
    "    Initialize the inference network, stream video to network,\n",
    "    and output stats and video.\n",
    "\n",
    "    :param args: Command line arguments parsed by `build_argparser()`\n",
    "    :param client: MQTT client\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #args.c = convert_color(args.c)\n",
    "    #args.ct = float(args.ct)\n",
    "    \n",
    "    # Initialise the class\n",
    "    infer_network = Network()\n",
    "\n",
    "    # Set Probability threshold for detections\n",
    "    prob_threshold = args.prob_threshold\n",
    "    \n",
    "    ### TODO: Load the model through `infer_network` ###\n",
    "    #infer_network.load_model(args.m, args.d, CPU_EXTENSION)\n",
    "    #net_input_shape = infer_network.get_input_shape()\n",
    "    \n",
    "    cur_request_id = 0\n",
    "    last_count = 0\n",
    "    total_count = 0\n",
    "    start_time = 0\n",
    "\n",
    "    n, c, h, w = infer_network.load_model(args.model,\n",
    "                                          args.device,\n",
    "                                          1,\n",
    "                                          1,\n",
    "                                          cur_request_id,\n",
    "                                          args.cpu_extension)[1]\n",
    "\n",
    "    ### TODO: Handle the input stream ###\n",
    "    \n",
    "    # Checks for live feed\n",
    "    if args.input == 'CAM':\n",
    "        input_stream = 0\n",
    "\n",
    "    # Checks for input image\n",
    "    elif args.input.endswith('.jpg') or args.input.endswith('.bmp') :\n",
    "        single_image_mode = True\n",
    "        input_stream = args.input\n",
    "\n",
    "    # Checks for video file\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "        assert os.path.isfile(args.input), \"Specified input file doesn't exist\"\n",
    "\n",
    "    cap = cv2.VideoCapture(input_stream)\n",
    "    \n",
    "    if input_stream:\n",
    "        cap.open(args.input)\n",
    "\n",
    "    ### TODO: Loop until stream is over ###\n",
    "    if not cap.isOpened():\n",
    "        log.error(\"ERROR! Unable to open video source\")\n",
    "        \n",
    "    global initial_w, initial_h, prob_threshold\n",
    "    prob_threshold = args.prob_threshold\n",
    "    initial_w = cap.get(3)\n",
    "    initial_h = cap.get(4)\n",
    "    while cap.isOpened():\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "        # Start async inference\n",
    "\n",
    "        ### TODO: Read from the video capture ###\n",
    "        image = cv2.resize(frame, (w, h))\n",
    "\n",
    "        ### TODO: Pre-process the image as needed ###\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = image.reshape((n, c, h, w))\n",
    "        \n",
    "\n",
    "        ### TODO: Start asynchronous inference for specified request ###\n",
    "        inf_start = time.time()\n",
    "        infer_network.exec_net(cur_request_id, image)\n",
    "\n",
    "        ### TODO: Wait for the result ###\n",
    "        if infer_network.wait(cur_request_id) == 0:\n",
    "            det_time = time.time() - inf_start\n",
    "            ### TODO: Get the results of the inference request ###\n",
    "            result = infer_network.get_output(cur_request_id)\n",
    "            if args.perf_counts:\n",
    "                perf_count = infer_network.performance_counter(cur_request_id)\n",
    "                performance_counts(perf_count)\n",
    "\n",
    "            frame, current_count = ssd_out(frame, result)\n",
    "            inf_time_message = \"Inference time: {:.3f}ms\"\\\n",
    "                               .format(det_time * 1000)\n",
    "            cv2.putText(frame, inf_time_message, (15, 15),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "\n",
    "            ### TODO: Extract any desired stats from the results ###\n",
    "\n",
    "            ### TODO: Calculate and send relevant information on ###\n",
    "            ### current_count, total_count and duration to the MQTT server ###\n",
    "            ### Topic \"person\": keys of \"count\" and \"total\" ###\n",
    "            ### Topic \"person/duration\": key of \"duration\" ###\n",
    "            if current_count > last_count:\n",
    "                start_time = time.time()\n",
    "                total_count = total_count + current_count - last_count\n",
    "                client.publish(\"person\", json.dumps({\"total\": total_count}))\n",
    "\n",
    "            # Person duration in the video is calculated\n",
    "            if current_count < last_count:\n",
    "                duration = int(time.time() - start_time)\n",
    "                # Publish messages to the MQTT server\n",
    "                client.publish(\"person/duration\",\n",
    "                               json.dumps({\"duration\": duration}))\n",
    "\n",
    "            client.publish(\"person\", json.dumps({\"count\": current_count}))\n",
    "            last_count = current_count\n",
    "\n",
    "            if key_pressed == 27:\n",
    "                break\n",
    "        ### TODO: Send the frame to the FFMPEG server ###\n",
    "        sys.stdout.buffer.write(frame)  \n",
    "        sys.stdout.flush()\n",
    "        ### TODO: Write an output image if `single_image_mode` ###\n",
    "        if single_image_mode:\n",
    "            cv2.imwrite('output_image.jpg', frame)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    client.disconnect()\n",
    "    infer_network.clean()\n",
    "##################################################\n",
    "\"\"\"\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    optional.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def infer_on_video(args):\n",
    "    # Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "\n",
    "    # Load the network model into the IE\n",
    "    plugin.load_model(args.m, args.d, CPU_EXTENSION)\n",
    "    net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "    counter = 0\n",
    "    incident_flag = False\n",
    "    # Process frames until the video ends, or process is exited\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "\n",
    "        # Pre-process the frame\n",
    "        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "        p_frame = p_frame.transpose((2,0,1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        plugin.async_inference(p_frame)\n",
    "\n",
    "        # Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            result = plugin.extract_output()\n",
    "            ### TODO: Process the output\n",
    "            incident_flag = assess_scene(result, counter, incident_flag)\n",
    "\n",
    "\n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def assess_scene(result, counter, incident_flag):\n",
    "    '''\n",
    "    Based on the determined situation, potentially send\n",
    "    a message to the pets to break it up.\n",
    "    '''\n",
    "    if result[0][1] == 1 and not incident_flag:\n",
    "        timestamp = counter / 30\n",
    "        print(\"Log: Incident at {:.2f} seconds.\".format(timestamp))\n",
    "        print(\"Break it up!\")\n",
    "        incident_flag = True\n",
    "    elif result[0][1] != 1:\n",
    "        incident_flag = False\n",
    "\n",
    "    return incident_flag\n",
    "\n",
    "\"\"\"\n",
    "##################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Load the network and parse the output.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Grab command line args\n",
    "    args = build_argparser().parse_args()\n",
    "    # Connect to the MQTT server\n",
    "    client = connect_mqtt()\n",
    "    # Perform inference on the input stream\n",
    "    infer_on_stream(args, client)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
